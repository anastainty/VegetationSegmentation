import torch
import numpy as np
from torch.utils.data import DataLoader
from tqdm import tqdm
from sklearn.metrics import confusion_matrix, classification_report
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from datetime import datetime

import config
from dataset import VegetationDataset
from model import UNet

# --- –ù–ê–°–¢–†–û–ô–ö–ò –ü–û–î –ù–û–í–£–Æ –ú–û–î–ï–õ–¨ ---
MODEL_FILENAME = "unet_best_f1_model.pth"
# –ü–æ—Ä—è–¥–æ–∫ –∫–ª–∞—Å—Å–æ–≤ –¥–æ–ª–∂–µ–Ω –°–¢–†–û–ì–û —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å train.py!
CLASSES = ["–§–æ–Ω", "–ê—Å—Ñ–∞–ª—å—Ç", "–¢—Ä–∞–≤–∞", "–î–µ—Ä–µ–≤—å—è", "–ö—É—Å—Ç—ã"]


def evaluate_model():
    print(f"üìä –ù–∞—á–∏–Ω–∞–µ–º –æ—Ü–µ–Ω–∫—É —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {config.DEVICE}")
    print(f"üì° –û–∂–∏–¥–∞–µ—Ç—Å—è –∫–∞–Ω–∞–ª–æ–≤: {config.NUM_CHANNELS} (RGB + DSM + NDVI)")

    # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞–ø–∫–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    # –ü–æ–¥–Ω–∏–º–∞–µ–º—Å—è –Ω–∞ —É—Ä–æ–≤–µ–Ω—å –≤—ã—à–µ src
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    results_dir = os.path.join(base_dir, "data", "processed", "results", timestamp)
    os.makedirs(results_dir, exist_ok=True)
    print(f"üìÇ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫—É: {results_dir}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—É—Ç–µ–π
    if not os.path.exists(config.IMAGE_PATH):
        print(f"‚ùå –û–®–ò–ë–ö–ê: –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {config.IMAGE_PATH}")
        return

    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–í–ê–ñ–ù–û: –î–æ–±–∞–≤–∏–ª–∏ ndvi_path)
    # –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ Dataset, —á—Ç–æ –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏, —á—Ç–æ–±—ã –Ω–∞—Ä–µ–∑–∞—Ç—å –ø–∞—Ç—á–∏
    roi_file = os.path.join(config.MASKS_DIR, "roi_mask.tif")

    val_dataset = VegetationDataset(
        image_path=config.IMAGE_PATH,
        dsm_path=config.DSM_PATH,
        ndvi_path=config.NDVI_PATH,  # <--- –í–ê–ñ–ù–û: NDVI
        mask_path=config.MASK_PATH,
        roi_path=roi_file if os.path.exists(roi_file) else None,
        patch_size=config.PATCH_SIZE,
        augment=False  # –î–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–µ –Ω—É–∂–Ω–∞
    )

    # Batch size –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –ø–æ–±–æ–ª—å—à–µ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=0)

    # 3. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    # –í–ê–ñ–ù–û: n_channels –±–µ—Ä–µ–º –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (—Ç–∞–º —Ç–µ–ø–µ—Ä—å 7)
    model = UNet(n_channels=config.NUM_CHANNELS, n_classes=config.NUM_CLASSES).to(config.DEVICE)

    # –ò—â–µ–º –º–æ–¥–µ–ª—å
    possible_paths = [
        MODEL_FILENAME,
        os.path.join("src", MODEL_FILENAME),
        os.path.join(base_dir, MODEL_FILENAME)
    ]

    current_model_path = None
    for p in possible_paths:
        if os.path.exists(p):
            current_model_path = p
            break

    if not current_model_path:
        print(f"‚ùå –û–®–ò–ë–ö–ê: –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏ {MODEL_FILENAME}.")
        return

    print(f"üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –∏–∑: {current_model_path}")
    state = torch.load(current_model_path, map_location=config.DEVICE)
    model.load_state_dict(state)
    model.eval()

    all_preds = []
    all_targets = []

    # 4. –ü—Ä–æ–≥–æ–Ω –¥–∞–Ω–Ω—ã—Ö
    print(f"–í—Å–µ–≥–æ –ø–∞—Ç—á–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏: {len(val_dataset)}")
    with torch.no_grad():
        for images, masks in tqdm(val_loader, desc="–°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏"):
            images = images.to(config.DEVICE)
            outputs = model(images)

            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            preds = torch.argmax(outputs, dim=1).cpu().numpy().flatten()
            targets = masks.cpu().numpy().flatten()

            # --- –í–ê–ñ–ù–û: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è ---
            # –ò—Å–∫–ª—é—á–∞–µ–º –ø–∏–∫—Å–µ–ª–∏ —Å–æ –∑–Ω–∞—á–µ–Ω–∏–µ–º 255 (—ç—Ç–æ –ø–∞–¥–¥–∏–Ω–≥ –∏–ª–∏ –≥—Ä–∞–Ω–∏—Ü—ã ROI)
            # –ï—Å–ª–∏ –∏—Ö –Ω–µ —É–±—Ä–∞—Ç—å, –æ–Ω–∏ –∏—Å–ø–æ—Ä—Ç—è—Ç –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫
            valid_mask = targets != 255

            if np.any(valid_mask):
                all_preds.extend(preds[valid_mask])
                all_targets.extend(targets[valid_mask])

    if len(all_targets) == 0:
        print("‚ùå –û—à–∏–±–∫–∞: –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –ø–∏–∫—Å–µ–ª–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ (–≤–æ–∑–º–æ–∂–Ω–æ, –≤—Å—è –º–∞—Å–∫–∞ = 255?)")
        return

    # 5. –†–∞—Å—á–µ—Ç –ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
    print("\nüßÆ –†–∞—Å—á–µ—Ç –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ –∏ IoU...")
    # labels=range(len(CLASSES)) –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –º–∞—Ç—Ä–∏—Ü–∞ –±—É–¥–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
    cm = confusion_matrix(all_targets, all_preds, labels=range(len(CLASSES)))

    # –°—á–∏—Ç–∞–µ–º IoU –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
    iou_scores = []
    print("\n--- IoU –ø–æ –∫–ª–∞—Å—Å–∞–º ---")
    for i in range(len(CLASSES)):
        tp = cm[i, i]
        fp = cm[:, i].sum() - tp
        fn = cm[i, :].sum() - tp
        denominator = tp + fp + fn

        if denominator > 0:
            iou = tp / denominator
        else:
            iou = 0.0  # –ï—Å–ª–∏ –∫–ª–∞—Å—Å–∞ –≤–æ–æ–±—â–µ –Ω–µ –±—ã–ª–æ –≤ –≤—ã–±–æ—Ä–∫–µ

        iou_scores.append(iou)
        print(f"{CLASSES[i]:<10}: {iou:.4f}")

    mean_iou = np.mean(iou_scores)
    print(f"{'-' * 20}\nMean IoU  : {mean_iou:.4f}")

    # 6. –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    print("üìà –ì–µ–Ω–µ—Ä–∞—Ü–∏—è CSV –æ—Ç—á–µ—Ç–∞...")
    report_dict = classification_report(
        all_targets, all_preds,
        target_names=CLASSES,
        labels=range(len(CLASSES)),
        output_dict=True,
        zero_division=0
    )
    df_report = pd.DataFrame(report_dict).transpose()

    # –î–æ–±–∞–≤–ª—è–µ–º IoU –≤ —Ç–∞–±–ª–∏—Ü—É
    df_report['IoU'] = np.nan
    for i, class_name in enumerate(CLASSES):
        if class_name in df_report.index:
            df_report.loc[class_name, 'IoU'] = iou_scores[i]

    if 'macro avg' in df_report.index:
        df_report.loc['macro avg', 'IoU'] = mean_iou

    print("\n=== –ü–û–õ–ù–´–ô –û–¢–ß–ï–¢ ===")
    print(df_report)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV
    csv_path = os.path.join(results_dir, "accuracy_report.csv")
    df_report.to_csv(csv_path)
    print(f"üìÑ –¢–∞–±–ª–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {csv_path}")

    # 7. –†–∏—Å—É–µ–º –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –∫—Ä–∞—Å–∏–≤—ã—Ö —Ü–≤–µ—Ç–æ–≤ (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö –æ—Ç –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞)
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    cm_normalized = np.nan_to_num(cm_normalized)  # –£–±–∏—Ä–∞–µ–º –¥–µ–ª–µ–Ω–∏–µ –Ω–∞ 0

    plt.figure(figsize=(10, 8))
    sns.heatmap(cm_normalized, annot=True, fmt=".2f", cmap="Greens",
                xticklabels=CLASSES, yticklabels=CLASSES)
    plt.title(f"Confusion Matrix (Normalized)\nMean IoU: {mean_iou:.4f}")
    plt.ylabel("–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å (Ground Truth)")
    plt.xlabel("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å (Prediction)")
    plt.tight_layout()

    plot_path = os.path.join(results_dir, "confusion_matrix.png")
    plt.savefig(plot_path, dpi=300)
    print(f"üñº –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {plot_path}")

    # 8. –¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å–∞–º–º–∞—Ä–∏
    with open(os.path.join(results_dir, "summary.txt"), "w") as f:
        f.write(f"Model: {current_model_path}\n")
        f.write(f"Date: {timestamp}\n")
        f.write(f"Mean IoU: {mean_iou:.4f}\n")
        f.write(f"Accuracy: {report_dict['accuracy']:.4f}\n")
        f.write("-" * 20 + "\n")
        for i, score in enumerate(iou_scores):
            f.write(f"{CLASSES[i]}: {score:.4f}\n")

    print("\n‚úÖ –û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")


if __name__ == "__main__":
    evaluate_model()